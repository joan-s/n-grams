{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0V5wnM_54gV"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import nltk\n",
        "nltk.download('cess_cat')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bD8byP7V54gb"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import cess_cat as corpus\n",
        "\n",
        "words = []\n",
        "words_to_remove = ['*0*', '-Fpa-', '-Fpt-']\n",
        "#for s in tqdm(corpus.sents()[:1000]): # debug or quickly train the network\n",
        "for s in tqdm(corpus.sents()):\n",
        "    new_s = ['<s>'] + s[:-1] + ['</s>']\n",
        "    new_s = [w for w in new_s if w not in words_to_remove]\n",
        "    words.extend(new_s)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyWhB_FG54ge"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class FixedWindow(Dataset):\n",
        "    def __init__(self, words, length_window):\n",
        "        super().__init__()\n",
        "        self.length_window = length_window\n",
        "        # TODO:\n",
        "        # compute the vocabulary = list of unique words in 'words',\n",
        "        # then assign a unique id number to each word in the\n",
        "        # vocabulary, \n",
        "        # and finally compute a list of ids, one per word in 'words'\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids) - self.length_window\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        #TODO:\n",
        "        # returns a pair of tensors (first_ids, last_id) where\n",
        "        # first_ids are the ids of the words starting at index\n",
        "        # idx with length length_window-1, and last_id is the\n",
        "        # id at position idx+self.length_window-1, next to\n",
        "        # first_ids\n",
        "        pass\n",
        "\n",
        "\n",
        "\n",
        "length_window = 5\n",
        "dataset = FixedWindow(words, length_window)\n",
        "\n",
        "x, y = dataset.__getitem__(10)\n",
        "#print('x = {}, y = {}'.format(x, y))\n",
        "\n",
        "batch_size = 1000 # 5 to debug\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True) # shuffle=False to debug\n",
        "\n",
        "if True:\n",
        "    for nbatch, (X, y) in enumerate(dataloader):\n",
        "        print('batch {}'.format(nbatch))\n",
        "        print('X = {}'.format(X))\n",
        "        print('y = {}'.format(y))\n",
        "        for x,z in zip(X.numpy(), y.numpy()):\n",
        "            print([dataset.id2word[w] for w in x], end=' ')\n",
        "            print(dataset.id2word[z])\n",
        "        if nbatch==3:\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTnVK2qA54gi"
      },
      "outputs": [],
      "source": [
        "class NNLM(nn.Module):\n",
        "    def __init__(self, num_classes, dim_input, dim_hidden, dim_embedding):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.dim_input = dim_input\n",
        "        self.dim_hidden = dim_hidden\n",
        "        self.dim_embedding = dim_embedding\n",
        "        self.embeddings = nn.Embedding(self.num_classes, self.dim_embedding) # embedding layer or look up table\n",
        "        self.hidden1 = nn.Linear(self.dim_input * self.dim_embedding, self.dim_hidden, bias=False)\n",
        "        self.ones = nn.Parameter(torch.ones(self.dim_hidden))       \n",
        "        self.hidden2 = nn.Linear(self.dim_hidden, self.num_classes, bias=False)\n",
        "        self.hidden3 = nn.Linear(self.dim_input * self.dim_embedding, self.num_classes, bias=False) # final layer\n",
        "        self.bias = nn.Parameter(torch.ones(self.num_classes))\n",
        "\n",
        "    def forward(self, X):\n",
        "        word_embeds = self.embeddings(X)\n",
        "        X = word_embeds.view(-1, self.dim_input * self.dim_embedding) # first layer\n",
        "        tanh = torch.tanh(self.ones + self.hidden1(X)) # tanh layer\n",
        "        output = self.bias + self.hidden3(X) + self.hidden2(tanh) # summing up all the layers with bias\n",
        "        return output\n",
        "\n",
        "\n",
        "num_classes = dataset.vocabulary_size\n",
        "dim_input = length_window - 1\n",
        "dim_hidden = 50\n",
        "dim_embedding = 32\n",
        "learning_rate= 1e-3\n",
        "num_epochs = 60\n",
        "\n",
        "model = NNLM(num_classes, dim_input, dim_hidden, dim_embedding)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "path = 'NNLM.pt'\n",
        "do_train = True\n",
        "do_test = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In the top menu go to Runtime -> Change runtime type and set Hardware \n",
        "# accelerator to GPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
        "print(device)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "ISE7W8MG9niv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pENOQtyI54gl"
      },
      "outputs": [],
      "source": [
        "from torch.cuda.random import device_count\n",
        "if do_train:\n",
        "    size = len(dataloader.dataset)\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch, (X, y) in enumerate(dataloader):\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            loss = loss_fn(pred, y)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if batch % 100 == 0:\n",
        "                loss, current = loss.item(), batch * batch_size\n",
        "                print('Epoch {} loss: {:>7f}  [{:>5d}/{:>5d}]'\n",
        "                    .format(epoch+1, loss, current, size))\n",
        "\n",
        "    torch.save({'model_state_dict': model.state_dict()}, path)\n",
        "else:\n",
        "    checkpoint = torch.load(path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nrstu81q54gm"
      },
      "outputs": [],
      "source": [
        "if do_test:\n",
        "    num_sentences = 5\n",
        "    max_num_words = 100\n",
        "\n",
        "    nsent = 0\n",
        "    generated_words = ['<s>', 'El', 'dia', 'que']\n",
        "    assert len(generated_words)==dim_input # length_window-1\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        while (nsent < num_sentences) and (len(generated_words) < max_num_words):\n",
        "            input_ids = None\n",
        "            # TODO: set input_ids to the list of ids of the last dim_input generated words\n",
        "\n",
        "            pred = model(torch.tensor(input_ids).unsqueeze(0).to(device))\n",
        "            probs = torch.nn.functional.softmax(pred, dim=1)\n",
        "\n",
        "            #TODO:\n",
        "            # probs is the probability of each id (word) in the vocabulary.\n",
        "            # Now you have to select one output_id according to them, either the one\n",
        "            # with maximum probability, or sample one id according to their\n",
        "            # distribution in probs.\n",
        "            # Hint: see doc. on numpy argmax and torch.multinomial\n",
        "            # What's better ?\n",
        "\n",
        "            output_word = None\n",
        "            # TODO: get the word for output_id\n",
        "\n",
        "            generated_words += [output_word]\n",
        "            if output_word=='</s>':\n",
        "                nsent += 1\n",
        "\n",
        "    generated_text = ' '.join(generated_words)\n",
        "    generated_text = generated_text.replace(' </s> <s>', '.').replace('<s> ','').replace(' </s>','.')\n",
        "    for s in [' l\\' ',' s\\' ',' d\\' ',]:\n",
        "        generated_text = generated_text.replace(s, s[:-1])\n",
        "    generated_text = generated_text.replace(' , ', ', ').replace('_',' ')\n",
        "    print(generated_text)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}