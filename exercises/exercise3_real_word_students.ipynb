{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQB5JHcy7-Hc"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lod33r6zuVKX"
      },
      "outputs": [],
      "source": [
        "words = None\n",
        "if False:\n",
        "    from nltk.corpus import gutenberg as corpus\n",
        "    words = corpus.words()\n",
        "    vocabulary = vocabulary[803:-2]\n",
        "    # first 803 and last 2 words are punctuation signs, numbers and underscored \n",
        "    # words like _home_\n",
        "else:\n",
        "    # Using the whole corpus means too much work to later compute the words at a \n",
        "    # distance for each word in the vocabulary, so we load just one book\n",
        "    import nltk\n",
        "    nltk.download('gutenberg')\n",
        "    words = nltk.corpus.gutenberg.words('austen-emma.txt')\n",
        "    # using all the words in the book will take 25-30 min. to process later\n",
        "    # so we limit its number for the moment.\n",
        "    words = words[:10000] # cell [15] will take 1.5 min. \n",
        "    #words = words[:100000] # cell [15] will take 15 min. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtZbq2hU7-Hj"
      },
      "outputs": [],
      "source": [
        "vocabulary = list(set(words))\n",
        "vocabulary.sort()\n",
        "# get rid of some non-words like ',' '--' '['\n",
        "idx_first_word = vocabulary.index('A')\n",
        "vocabulary = vocabulary[idx_first_word:]\n",
        "# plus some more annoying non-words\n",
        "vocabulary.remove('[')\n",
        "vocabulary.remove(']')\n",
        "vocabulary.remove('`')\n",
        "vocabulary.remove('II')\n",
        "vocabulary = np.array(vocabulary)\n",
        "print(vocabulary)\n",
        "print('Vocabulary size {} words'.format(len(vocabulary)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3CoMFvT7-Hk"
      },
      "source": [
        "For each word in the vocabulary find the nearest words = at Levenshtein distance up to ``MAX_DIST``. This is a long computation, $O(n^2)$ for $n$ size of the vocabulary. We try to speed up it a little : if $\\text{dist}(w_1, w_2) \\leq d$ then $|\\text{len}(w_1) - \\text{len}(w_2)| \\leq d$. This reduces the candidate words in the vocabulary for which to compute the distance to each word.\n",
        "We save the resulting dictionary to avoid recomputing it each time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQSBCKoC7-Hm"
      },
      "outputs": [],
      "source": [
        "from nltk.metrics.distance import edit_distance\n",
        "\n",
        "def levenshtein(s1, s2):\n",
        "    return edit_distance(s1, s2, substitution_cost=1, transpositions=True)\n",
        "\n",
        "word_lengths = np.array([len(w) for w in vocabulary])\n",
        "dict_lengths = {}\n",
        "for l in range(min(word_lengths), max(word_lengths)+1):\n",
        "    dict_lengths[l] = vocabulary[word_lengths==l] # needs vocabulary to be a numpy array\n",
        "\n",
        "min_length = min(dict_lengths.keys())\n",
        "max_length = max(dict_lengths.keys())\n",
        "\n",
        "MAX_DIST = 1\n",
        "fname = 'close_words_{}.pkl'.format(MAX_DIST)\n",
        "if not os.path.exists(fname):\n",
        "    close_words = {}\n",
        "    for word in tqdm(vocabulary):\n",
        "        length = len(word)\n",
        "        candidate_words = []\n",
        "        d1 = max(min_length, length - MAX_DIST)\n",
        "        d2 = min(max_length, length + MAX_DIST)\n",
        "        for d in range(d1, d2+1):\n",
        "            candidate_words.extend(dict_lengths[d])\n",
        "        close_words[word] = [w for w in candidate_words if levenshtein(word,w) <= MAX_DIST]\n",
        "\n",
        "    with open(fname,'wb') as f:\n",
        "        pickle.dump(close_words, f)\n",
        "else:\n",
        "    close_words = pickle.load(open(fname,'rb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIdE_y_Z7-Ho"
      },
      "source": [
        "Given one sentence $X$, which is a list of words, build the candidates to correct sentence $C(X)$ assuming at most one word is mispelled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-GOhyl37-Hp"
      },
      "outputs": [],
      "source": [
        "#sentence = 'Only two of the apples'\n",
        "sentence = 'I wish you where here'\n",
        "X = sentence.split(' ')\n",
        "for x in X:\n",
        "  assert x in vocabulary, 'All the words in the sentence must belong to the '\\\n",
        "      + 'vocabulary, {} doesn\\'t'.format(x)\n",
        "CX = [X] # no errors\n",
        "for i in range(len(X)): # one mispelled word at a time\n",
        "    if X[i] in vocabulary: \n",
        "        for cw in close_words[X[i]]:\n",
        "            if cw != X[i]:\n",
        "                C = X.copy()\n",
        "                C[i] = cw\n",
        "                CX.append(C)\n",
        "    else:\n",
        "        pass # let it be as is\n",
        "for W in CX:\n",
        "    print('\\t'.join(W))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gT9-f0od7-Hr"
      },
      "source": [
        "Likelihood $P(X | W) = \\prod_{i=1}^n p(x_i | w_i)$ where $n$ is number of words in $X$ (same as in $W$), and $p(x | w)$  is Eq. B.8. $X$ is the written sentence, $W$ are the candidate sentences in $C(X)$. Each $W$ contains zero (ie, $W=X$) or at most one mispelled word, and in this case the mispelled word $w_i$ is at a Levenshtein distance $1...$ ``MAX_DIST`` of the written word $x_i$ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WF4RPnLX7-Hs"
      },
      "outputs": [],
      "source": [
        "alpha = 0.95\n",
        "likelihoods = []\n",
        "for W in CX:\n",
        "    PXW = 1.0\n",
        "    #print(X,W)\n",
        "    for x,w in zip(X,W):\n",
        "        if w==x:\n",
        "            pxw = alpha\n",
        "        else:\n",
        "            close_to_x = close_words[x] # includes x itself\n",
        "            pxw = (1-alpha) / (len(close_to_x) - 1) # so we substract 1\n",
        "        PXW *= pxw\n",
        "    likelihoods.append(PXW)\n",
        "\n",
        "likelihoods = np.array(likelihoods)\n",
        "idx_most_likely = likelihoods.argmax()\n",
        "print('Sentence with highest likelihood is the written one, X')\n",
        "print('\\t'.join(CX[idx_most_likely]), '\\t', likelihoods[idx_most_likely])\n",
        "print('Likelihoods')\n",
        "num_candidates = len(CX)\n",
        "for i in range(num_candidates):\n",
        "    print('\\t'.join(CX[i]), '\\t', likelihoods[i])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fL_jue5E7-Hv"
      },
      "source": [
        "Priors $P(W)$ for all $W \\in C(X)$ computed by a LM model, for instance tri-grams (on this same corpus). We've chosen the stupid backoff version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64PapwMh7-Hw"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt')\n",
        "\n",
        "sents = nltk.corpus.gutenberg.sents('austen-emma.txt')\n",
        "text = []\n",
        "for s in tqdm(sents):\n",
        "    text.append(s[:-1]) # except ending point\n",
        "\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.lm.models import StupidBackoff\n",
        "\n",
        "n=3\n",
        "lm = StupidBackoff(alpha=0.4, order=n)\n",
        "train, vocab = padded_everygram_pipeline(n, text)\n",
        "lm.fit(train, vocab)\n",
        "print(lm.counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtUBjkAl7-Hy"
      },
      "outputs": [],
      "source": [
        "def P(W, verbose=False):\n",
        "    S = ['<s>', '<s>',] + W + ['</s>'] \n",
        "    if verbose: print(S)\n",
        "    num_words = len(S)\n",
        "    PW = 1.0\n",
        "    for i in range(2,num_words-1): # omit </s> because likelihoods don't have it\n",
        "        score = lm.score(S[i], [S[i-2], S[i-1]])\n",
        "        if verbose: print('P({} | {}, {}) = {}'.format(S[i], S[i-2], S[i-1], score))\n",
        "        PW *= score\n",
        "    return PW\n",
        "\n",
        "print('X =', X)\n",
        "score = P(X, verbose=True)\n",
        "print('P(X) = {}'.format(score))\n",
        "print('\\nW = CX[1] = {}'.format(CX[1]))\n",
        "score = P(CX[1], verbose=True)\n",
        "print('P(W) = {}'.format(score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbPXfszT7-Hz"
      },
      "outputs": [],
      "source": [
        "priors = []\n",
        "for W in CX:\n",
        "    priors.append(P(W))\n",
        "    \n",
        "posteriors = np.array(priors)*np.array(likelihoods)\n",
        "idx_best_post = np.argmax(posteriors)\n",
        "\n",
        "for i in range(num_candidates):\n",
        "    best = '<----' if i==idx_best_post else ''\n",
        "    print('\\t'.join(CX[i]), '\\tlikelihood={}\\tprior={}\\tposterior={} {}'\n",
        "          .format(likelihoods[i], priors[i], posteriors[i],best))\n",
        "\n",
        "print('\\nThe original sentence was')\n",
        "print('\\t' + ' '.join(X))\n",
        "print('The right sentence is')\n",
        "print('\\t' + ' '.join(CX[idx_best_post]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's it! Now, let's try this code and do some modifications:\n",
        "1. change probabilities by log of probabilities to get scores more easy to interpret\n",
        "1. relax the assumption that in a sentence there's at most 1 mispelled word, what if we suppose there may be at most 2 ?\n",
        "1. and what is the effect of $\\alpha$ ? change it to 0.9, 0.8...\n",
        "1. make a long (>20) list of sentences each with 1 or 2 mispelled words to assess the speller : does it really work ? \n",
        "1. same for a second list of sentences withouth any spelling error (but different from any one in the selected part of the book), are there any errors ?\n",
        "1. extra points: what if you change the tri-gram language model by the neural network of exercise 2 ?"
      ],
      "metadata": {
        "id": "hFbhoCLL47zW"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ngrams",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "a5f976fab99073dff0a6bed391851c6449ba69de3700e06fc73b9df6f40e665b"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}