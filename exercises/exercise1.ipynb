{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1e662c74",
      "metadata": {
        "id": "1e662c74"
      },
      "source": [
        "Load a corpus in Catalan or English. The nltk corpora result from tokenizing and segmenting into sentences large collections of text.\n",
        "\n",
        "The ``gutenberg`` corpus comes from a set of English literature classics. The ``cess_cat`` corpus comes from https://www.cs.upc.edu/~nlp/wikicorpus/, the \"120 Million Word Spanish Corpus\" which has a subset in Catalan of 50 million words scrapped from Vikipedia in 2006."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OsV6sILlovc_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsV6sILlovc_",
        "outputId": "dcabacd6-bfa7-46dd-80e6-f43e006d6c1e"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('cess_cat')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1525dd55",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1525dd55",
        "outputId": "c9320174-5473-4fcf-925c-991ac3d5e1b4"
      },
      "outputs": [],
      "source": [
        "name_corpus = 'cess_cat'\n",
        "\n",
        "if name_corpus=='cess_cat':\n",
        "    from nltk.corpus import cess_cat as corpus\n",
        "    # clean the corpus of strange words\n",
        "    words = []\n",
        "    words_to_remove = ['*0*', '-Fpa-', '-Fpt-']\n",
        "    for w in tqdm(corpus.words()):\n",
        "        if w not in words_to_remove:\n",
        "            words.append(w)\n",
        "\n",
        "elif name_corpus=='gutenberg':\n",
        "    from nltk.corpus import gutenberg as corpus\n",
        "    print(corpus.fileids())\n",
        "    words = corpus.words()\n",
        "else:\n",
        "    assert False\n",
        "\n",
        "print('corpus {} : {} words, {} sentences'\n",
        "      .format(name_corpus, len(words), len(corpus.sents())))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd42b3ec",
      "metadata": {
        "id": "cd42b3ec"
      },
      "source": [
        "Build a language model from bigrams. A LM is just a dictionary\n",
        "with key = condition = one word, and value = ``FreqDist`` \n",
        "object = another dictionary with key = next word, value = number \n",
        "of occurrences.\n",
        "This is adapted from https://www.nltk.org/book/ch02.html, section 2.4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4ba30c3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4ba30c3",
        "outputId": "f7c31f39-3c43-41e9-e21b-b5a802d21a43"
      },
      "outputs": [],
      "source": [
        "grams = list(nltk.bigrams(words))\n",
        "# also trigrams, ngrams, everygrams(max_len)\n",
        "cfd = nltk.ConditionalFreqDist(grams)\n",
        "print(cfd.conditions())\n",
        "for i in [100, 200, 300, 400]:\n",
        "    print(cfd.conditions()[i])\n",
        "    print(cfd[cfd.conditions()[i]].most_common())\n",
        "    print('--------------')\n",
        "\n",
        "if name_corpus == 'cess_cat':\n",
        "    freq_dist =cfd['Una']\n",
        "else:\n",
        "    freq_dist = cfd['The']\n",
        "\n",
        "print(freq_dist.items())\n",
        "print(freq_dist.max())\n",
        "print(list(freq_dist.elements()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f271edc2",
      "metadata": {
        "id": "f271edc2"
      },
      "source": [
        "Sample text from the language model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b536851",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b536851",
        "outputId": "ab179ea8-588f-4531-e66d-e159910ee168"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def sample_bigram_model(cfd_bigrams, last_word, num=15):\n",
        "    for i in range(num):\n",
        "        print(last_word, end=' ')\n",
        "        # if we do w_k = \\arg \\max w \\in V p(w | w_{k-1}) with\n",
        "        #     next_word = cfdist[word].max()\n",
        "        # we get caught in a cycle, repeating again and again \n",
        "        # the same few words. It is better to sample from the\n",
        "        # probability distribution with\n",
        "        next_word = random.choice(list(cfd_bigrams[last_word].elements()))\n",
        "        last_word = next_word\n",
        "\n",
        "\n",
        "if name_corpus=='cess_cat':\n",
        "    print(sample_bigram_model(cfd, 'El', 100))\n",
        "    print(sample_bigram_model(cfd, 'La', 100))\n",
        "    print(sample_bigram_model(cfd, 'Per', 100))\n",
        "else:\n",
        "    print(sample_bigram_model(cfd, 'The', 100))\n",
        "    print(sample_bigram_model(cfd, 'For', 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "O4xuRzAqtfeZ",
      "metadata": {
        "id": "O4xuRzAqtfeZ"
      },
      "source": [
        "Extension of previous function to tri, 4... n-grams is long and complicated\n",
        "because conditions of cfd are not one word but lists of pairs, triplets, n-1 words. In addition, the probability of not finding the previous 2, 3..n\n",
        "generated words among the conditions (ngrams) is very high. So better rely\n",
        "on the ``lm`` package of nltk. It has also support for adding ``<s>``, ``</s>`` symbols to sentences (padding), different types of smoothing and backoff, and sampling text.\n",
        "\n",
        "Build a proper language model with support for ``<s>``, ``</s>``, smoothing, backoff, sampling and computation of perplexity. See how here\n",
        "https://www.nltk.org/api/nltk.lm.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bf4ae6e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bf4ae6e",
        "outputId": "6ed71b25-74cb-4365-b3fd-47f93ad02862"
      },
      "outputs": [],
      "source": [
        "if name_corpus=='cess_cat':\n",
        "    text = []\n",
        "    words_to_remove = ['*0*', '-Fpa-', '-Fpt-']\n",
        "    #for s in tqdm(corpus.sents()[:1000]): # debug or quickly train the network\n",
        "    for s in tqdm(corpus.sents()):\n",
        "        new_s = [w for w in s if w not in words_to_remove]\n",
        "        text.append(new_s[:-1]) # except ending point\n",
        "else:\n",
        "    text = []\n",
        "    for s in tqdm(corpus.sents()):\n",
        "        text.append(s[:-1]) # except ending point\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "cc058acc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<nltk.lm.models.MLE object at 0x7f3e19f6ada0>\n",
            "<s> <s> <s> Aquí , es riuen de nosaltres \" </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "Generated text\n",
            "Aquí , es riuen de nosaltres \" . El dret al vot \" . El fiscal demana penes de 28 i 11 anys de presó , vuit anys i sis mesos de presó i 120.000 pessetes de multa un agricultor que tenia emmagatzemats purins a la_seva granja .\n",
            "-------------\n",
            "<nltk.lm.models.Laplace object at 0x7f3e523d2050>\n",
            "<s> <s> <s> Arroyo va eludir relacionar els nous sortejos per millorar la imatge d' unitat , malgrat les darreres pluges , que han refusat obertament la invitació per considerar -lo un organisme compensatori pel transvasament de l' Ebre que es contempla al PHN ' </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "Generated text\n",
            "Arroyo va eludir relacionar els nous sortejos per millorar la imatge d' unitat , malgrat les darreres pluges , que han refusat obertament la invitació per considerar -lo un organisme compensatori pel transvasament de l' Ebre que es contempla al PHN ' . Amb l' operació , la companyia va encarir cinc pessetes els paquets de preus i que la futura Constitució europea no esmenti les regions en el_seu articulat , i ha precisat també que \" la sinistralitat a les mines del Gran_Llac_dels_Óssos va alimentar tot el programa de l' Inem ' . Fa d' amo d' una botiga .\n",
            "-------------\n",
            "<nltk.lm.models.StupidBackoff object at 0x7f3e51f837c0>\n",
            "<s> <s> <s> Aquí , es riuen de nosaltres \" </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "Generated text\n",
            "Aquí , es riuen de nosaltres \" . En aquest sentit , va afirmar ahir que el Regne_Unit i Espanya \" han trepitjat \" la sobirania de Gibraltar amb el Govern de l' Havana va considerar ahir que hi ha a la comarca d' Osona han organitzat un homenatge a Mossèn_Antoni_Pladevall_i_Font . L' apartat sobre Badalona especifica que es confirma l' existència d' un grup poderós de països , encapçalat per Alemanya , per retallar dràsticament les ajudes comunitàries al cultiu del lli que , una_vegada superats els mínims històrics , podria tornar a arribar a la platja , i la veritat és que la història ens demostra el que no està ni amb Arzalluz ni amb els bisbes \" .\n",
            "-------------\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.lm.models import MLE, Laplace, StupidBackoff\n",
        "\n",
        "n = 4\n",
        "lm1 = MLE(n)\n",
        "lm2 = Laplace(n)\n",
        "lm3 = StupidBackoff(alpha=0.4, order=n)\n",
        "for lm in [lm1, lm2, lm3]:\n",
        "    print(lm)\n",
        "    train, vocab = padded_everygram_pipeline(n, text)\n",
        "    # can not reuse the same pair of train, vocab!\n",
        "\n",
        "    # train is a generator of n-grams\n",
        "    #   print('train', list(list(train)[0])[:20])\n",
        "    # once printed can not be used again!\n",
        "    # vocab is the sentences in text padded with <s>, </s> and\n",
        "    # put in a single list\n",
        "    #   print('vocab', list(vocab)[:30])\n",
        "   \n",
        "    # this won't do: \n",
        "    #   lm.fit(train, vocab)\n",
        "    #   print(' '.join(lm.generate(100, random_seed=4)))\n",
        "    #\n",
        "    # because after </s> the language model keeps generating </s> all the time.\n",
        "    # The reason is explained here for the case of tri-grams:\n",
        "    # https://stackoverflow.com/questions/60295058/nltk-mle-model-clarification-trigrams-and-greater\n",
        "    # In short:\n",
        "    # Each sentence is completely independent of each other. The model doesn't know what came \n",
        "    # before that sentence nor what comes after. Also, remember that you're training a trigram \n",
        "    # model, so the last two words in every sentence are ('</s>', '</s>'). Therefore, the model \n",
        "    # learns that '</s>' is followed by '</s>' with a very high probability but it never learns \n",
        "    # that '</s>' can sometimes be followed by '<s>'.\n",
        "    # So the easiest solution to your problem is just to manually start a new sentence (i.e. \n",
        "    # call generate() again) every time you see '</s>'\n",
        "\n",
        "    lm.fit(train, vocab)\n",
        "    num_sentences_to_generate = 3\n",
        "    sentences = []\n",
        "    generated_words = None\n",
        "    for i in range(num_sentences_to_generate):\n",
        "        # this produces always the same sentence\n",
        "        #   generated_words = lm.generate(100, random_seed=4) \n",
        "        # this produces sentences that don't start like a sentence\n",
        "        #   generated_words = lm.generate(100, text_seed='<s>')\n",
        "        # this also returns always the same sentence\n",
        "        #   generated_words = lm.generate(100, text_seed='<s>', random_seed=4) \n",
        "\n",
        "        # instead, make first sentence with seed '<s>' and following ones\n",
        "        # with the last words of previous sentence plus </s> <s>\n",
        "        if i==0: # first sentence\n",
        "            generated_words = lm.generate(100, text_seed='<s>', random_seed=4) \n",
        "            # get many words, more than those in a sentence\n",
        "        else: # 2nd, 3rd... sentences are generated conditioned to a context=ending of previous one\n",
        "            text_seed = new_sentence[-n+1:] + ['</s>',] + ['<s>',]\n",
        "            # print('text seed', text_seed)\n",
        "            generated_words = lm.generate(100, text_seed=text_seed)\n",
        "        \n",
        "\n",
        "        # print('generated words', generated_words)\n",
        "        new_sentence = [w for w in generated_words if w not in ['<s>', '</s>']]\n",
        "        # print('new sentence', new_sentence)\n",
        "        sentences += new_sentence + [\".\"]\n",
        "\n",
        "    print('Generated text')\n",
        "    print(' '.join(sentences))\n",
        "    print('-------------')\n",
        "    \n",
        "    #print(lm.vocab.lookup(text[0]))\n",
        "    #print(lm.vocab.lookup(['beeeee', 'muuuu']))\n",
        "    #print(lm.counts)\n",
        "    #print(lm.score('El'), lm.score('el'), lm.score('dia'), lm.score(\"<UNK>\"))\n",
        "    #print(lm.perplexity([('relació', 'amb', 'les', 'empreses')]))\n",
        "    "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
