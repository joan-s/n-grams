{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YQB5JHcy7-Hc"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SGTgkKv7-Hg",
        "outputId": "ba7eb48d-e945-4143-af89-6c5159b3aab8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#from nltk.corpus import gutenberg as corpus\n",
        "#words = corpus.words()\n",
        "#vocabulary = vocabulary[803:-2]\n",
        "##first 803 and last 2 words are punctuation signs, numbers and underscored words like _home_\n",
        "# too much works to later compute the words at a distance for each word in the vocabulary\n",
        "# so we load just one book\n",
        "\n",
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "words = nltk.corpus.gutenberg.words('austen-emma.txt')\n",
        "words = words[:4000] # debug, all the words will take 25-30 min. to process later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtZbq2hU7-Hj",
        "outputId": "e0aed681-1300-415d-8a6b-d4e4f2b7f243"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['A' 'Ah' 'All' 'And' 'At' 'Austen' 'Being' 'Between' 'Broadway'\n",
            " 'Brunswick' 'But' 'By' 'CHAPTER' 'Christmas' 'Dear' 'Depend' 'Dirty'\n",
            " 'Elton' 'Emma' 'Especially' 'Even' 'Ever' 'Every' 'Farmer' 'Hannah'\n",
            " 'Hartfield' 'He' 'Her' 'Highbury' 'His' 'How' 'I' 'If' 'Invite'\n",
            " 'Isabella' 'It' 'James' 'Jane' 'Knightley' 'Lane' 'London' 'Look'\n",
            " 'Matrimony' 'Miss' 'Mitchell' 'Mr' 'Mrs' 'My' 'No' 'Nobody' 'Not'\n",
            " 'November' 'October' 'Oh' 'Only' 'Poor' 'Pray' 'Randalls' 'She' 'Sixteen'\n",
            " 'Some' 'Sorrow' 'Square' 'Success' 'Taylor' 'That' 'The' 'There' 'They'\n",
            " 'This' 'Tis' 'VOLUME' 'We' 'Well' 'Weston' 'What' 'When' 'Whenever'\n",
            " 'Where' 'Who' 'With' 'Woodhouse' 'Woodhouses' 'You' 'Your' '_We_' '_one_'\n",
            " '_that_' '_them_' '_will_' '_you_' 'a' 'able' 'about' 'absence'\n",
            " 'acceptable' 'accepted' 'account' 'acquaintance' 'activity' 'actual'\n",
            " 'advantage' 'advantages' 'affection' 'affectionate' 'afforded' 'afraid'\n",
            " 'after' 'afterwards' 'again' 'age' 'ages' 'ago' 'agree' 'agreeable' 'all'\n",
            " 'allow' 'allowed' 'alloy' 'almost' 'alone' 'already' 'always' 'am'\n",
            " 'amiable' 'among' 'amounting' 'amuse' 'amusement' 'an' 'and' 'animated'\n",
            " 'another' 'answered' 'any' 'anywhere' 'apart' 'are' 'arose' 'as' 'ask'\n",
            " 'asks' 'at' 'attach' 'attached' 'attacked' 'attention' 'authority'\n",
            " 'aware' 'away' 'awoke' 'back' 'backgammon' 'bangs' 'be' 'bear' 'bears'\n",
            " 'beautiful' 'because' 'been' 'before' 'began' 'begin' 'behave' 'behaved'\n",
            " 'being' 'believe' 'believed' 'belong' 'beloved' 'best' 'better' 'between'\n",
            " 'beyond' 'black' 'blessed' 'blessings' 'body' 'born' 'borrowed' 'both'\n",
            " 'break' 'breakfast' 'bride' 'bring' 'brother' 'brought' 'business' 'but'\n",
            " 'by' 'bye' 'call' 'came' 'can' 'cannot' 'care' 'caresses' 'carriage'\n",
            " 'catch' 'ceased' 'certainly' 'chances' 'change' 'character' 'charmingly'\n",
            " 'chatted' 'cheer' 'cheerful' 'cheerfully' 'chicken' 'chiefly' 'childhood'\n",
            " 'children' 'chuse' 'circle' 'circumstance' 'civil' 'claim' 'clever'\n",
            " 'cleverer' 'cold' 'come' 'comes' 'comfort' 'comfortable' 'comfortably'\n",
            " 'coming' 'companion' 'comparatively' 'compassion' 'composed' 'comprehend'\n",
            " 'concerns' 'congratulations' 'connected' 'connexions' 'consciousness'\n",
            " 'consequence' 'considerable' 'considering' 'constantly' 'constitution'\n",
            " 'continuance' 'conversation' 'could' 'creature' 'cried' 'curtseys'\n",
            " 'daily' 'damp' 'danger' 'dare' 'darted' 'daughter' 'daughters' 'day'\n",
            " 'days' 'deal' 'dear' 'dearer' 'dearest' 'dearly' 'deathbed' 'debt'\n",
            " 'delicately' 'denying' 'depend' 'dependence' 'depressed' 'deserves'\n",
            " 'devoted' 'did' 'died' 'difference' 'differently' 'dine' 'dinner'\n",
            " 'directed' 'directly' 'dirty' 'disadvantages' 'disagreeable' 'disparity'\n",
            " 'disposed' 'disposition' 'distance' 'distress' 'divided' 'do' 'doing'\n",
            " 'domestic' 'done' 'door' 'doubt' 'draw' 'drawn' 'dreadfully' 'drizzle'\n",
            " 'each' 'early' 'easily' 'easy' 'eight' 'either' 'elder' 'else'\n",
            " 'employment' 'encouragements' 'endeavour' 'endeavouring' 'enjoyments'\n",
            " 'enough' 'entirely' 'equal' 'equals' 'esteeming' 'even' 'evening' 'event'\n",
            " 'ever' 'every' 'everywhere' 'evil' 'evils' 'exactly' 'excellent'\n",
            " 'exertions' 'existence' 'face' 'fact' 'fallen' 'family' 'fanciful' 'far'\n",
            " 'father' 'fault' 'faults' 'feel' 'feeling' 'felt' 'few' 'fill' 'find'\n",
            " 'fire' 'first' 'fish' 'fitted' 'five' 'flatter' 'flow' 'followed' 'fond'\n",
            " 'fondly' 'footing' 'for' 'foretell' 'forgotten' 'fortune' 'found' 'four'\n",
            " 'frequent' 'friend' 'friendliness' 'friends' 'friendship' 'from' 'gainer'\n",
            " 'gallantry' 'generations' 'generous' 'gentle' 'get' 'girl' 'give' 'given'\n",
            " 'glad' 'go' 'goes' 'going' 'gone' 'good' 'got' 'governess' 'gratefully'\n",
            " 'gratitude' 'great' 'greatest' 'grief' 'grievously' 'guess' 'habits'\n",
            " 'had' 'half' 'hands' 'handsome' 'happier' 'happily' 'happiness' 'happy'\n",
            " 'hard' 'hardly' 'harm' 'has' 'hating' 'have' 'having' 'he' 'head'\n",
            " 'health' 'hear' 'hearing' 'heart' 'hearted' 'help' 'her' 'here' 'hers'\n",
            " 'herself' 'highly' 'him' 'himself' 'his' 'hold' 'home' 'hope' 'hoped'\n",
            " 'horrible' 'horses' 'hour' 'house' 'housemaid' 'how' 'however' 'humoured'\n",
            " 'humours' 'hurry' 'husband' 'idea' 'ideas' 'idle' 'if' 'illnesses'\n",
            " 'imagine' 'immediately' 'important' 'impose' 'impossible' 'in'\n",
            " 'increased' 'indeed' 'independence' 'indistinct' 'indulgent' 'informed'\n",
            " 'inquiries' 'instance' 'intellectual' 'intelligent' 'intercourse'\n",
            " 'interested' 'interference' 'intimacy' 'intimate' 'into' 'is' 'it' 'its'\n",
            " 'joining' 'joke' 'joy' 'judgment' 'just' 'keep' 'kind' 'kindness' 'knew'\n",
            " 'know' 'knowing' 'known' 'knows' 'lady' 'large' 'last' 'late' 'laughing'\n",
            " 'lawn' 'leave' 'left' 'less' 'let' 'letting' 'lieu' 'life' 'like' 'liked'\n",
            " 'likely' 'little' 'live' 'lived' 'living' 'lock' 'long' 'longer' 'look'\n",
            " 'looked' 'looks' 'lose' 'loss' 'lost' 'loved' 'loves' 'luck' 'lucky'\n",
            " 'made' 'maintain' 'make' 'making' 'man' 'manage' 'manner' 'manners'\n",
            " 'many' 'marriage' 'married' 'marry' 'marrying' 'match' 'matches'\n",
            " 'matrimony' 'matter' 'matters' 'may' 'me' 'mean' 'means' 'meant' 'meet'\n",
            " 'meeting' 'melancholy' 'mentioned' 'merely' 'merit' 'met' 'might' 'mild'\n",
            " 'mildness' 'mile' 'miles' 'mind' 'misfortunes' 'miss' 'mistress'\n",
            " 'moonlight' 'more' 'morning' 'most' 'mother' 'mournful' 'much' 'must'\n",
            " 'mutual' 'mutually' 'my' 'myself' 'name' 'native' 'natural' 'nearly'\n",
            " 'necessary' 'need' 'needlework' 'nervous' 'never' 'next' 'night' 'no'\n",
            " 'nobody' 'nominal' 'none' 'nonsense' 'nor' 'not' 'nothing' 'now' 'nursed'\n",
            " 'obliged' 'observe' 'observed' 'occupied' 'odd' 'of' 'off' 'office'\n",
            " 'often' 'old' 'older' 'on' 'one' 'only' 'open' 'opinion' 'or' 'origin'\n",
            " 'other' 'others' 'our' 'out' 'over' 'owing' 'own' 'pain' 'papa' 'part'\n",
            " 'particularly' 'pass' 'passed' 'past' 'pay' 'paying' 'peculiarly'\n",
            " 'people' 'perfect' 'perfectly' 'period' 'persons' 'pictures' 'pity'\n",
            " 'place' 'placed' 'planned' 'planning' 'played' 'playful' 'playfully'\n",
            " 'pleasant' 'please' 'pleasure' 'poor' 'populous' 'possessed' 'possibly'\n",
            " 'power' 'powers' 'pray' 'present' 'pretty' 'promise' 'promoted'\n",
            " 'properly' 'prospect' 'proud' 'proved' 'provision' 'punctual' 'put'\n",
            " 'quarrel' 'question' 'quite' 'rain' 'rained' 'rank' 'rate' 'rather'\n",
            " 'rational' 'reach' 'real' 'really' 'recalled' 'recollection'\n",
            " 'recommended' 'reconciled' 'reflection' 'regard' 'regrets' 'rejoined'\n",
            " 'remembrance' 'removed' 'replied' 'required' 'respectable' 'rest'\n",
            " 'restraint' 'returned' 'rich' 'right' 'rising' 's' 'sad' 'safely' 'said'\n",
            " 'same' 'sat' 'satisfaction' 'satisfactorily' 'say' 'saying' 'scheme'\n",
            " 'secure' 'see' 'seemed' 'seen' 'self' 'selfishness' 'sensible' 'separate'\n",
            " 'servant' 'service' 'settled' 'seven' 'shadow' 'shall' 'shame' 'shape'\n",
            " 'she' 'shew' 'shocking' 'shoes' 'shook' 'short' 'should' 'shrubberies'\n",
            " 'sigh' 'silly' 'since' 'single' 'sir' 'sister' 'sisters' 'sit'\n",
            " 'situation' 'six' 'sixteen' 'sleep' 'slighted' 'smiled' 'smiles'\n",
            " 'smoothed' 'so' 'society' 'solemn' 'solitude' 'some' 'somebody'\n",
            " 'something' 'sometimes' 'son' 'soon' 'sorrow' 'sorry' 'sort' 'spared'\n",
            " 'speak' 'speck' 'spend' 'spent' 'spirits' 'spite' 'spoken' 'stable'\n",
            " 'straightforward' 'struggled' 'subject' 'success' 'such' 'suffering'\n",
            " 'suitable' 'supplied' 'support' 'suppose' 'supposes' 'sure' 'surprising'\n",
            " 'suspect' 'table' 'take' 'talent' 'talents' 'talk' 'talked' 'taught'\n",
            " 'tea' 'tear' 'tears' 'tell' 'temper' 'tenderer' 'than' 'that' 'the'\n",
            " 'their' 'them' 'then' 'there' 'therefore' 'these' 'they' 'thing' 'things'\n",
            " 'think' 'thinks' 'third' 'thirty' 'this' 'thoroughly' 'those' 'though'\n",
            " 'thought' 'thoughts' 'threatened' 'three' 'through' 'till' 'time' 'times'\n",
            " 'to' 'together' 'told' 'tolerably' 'too' 'town' 'triumph' 'troublesome'\n",
            " 'true' 'turned' 'turns' 'twenty' 'two' 'umbrellas' 'unaffected' 'uncle'\n",
            " 'understand' 'understanding' 'unexceptionable' 'unite' 'universally'\n",
            " 'unnecessary' 'unperceived' 'unreserve' 'up' 'upon' 'us' 'used' 'useful'\n",
            " 'usual' 'valetudinarian' 'various' 'vast' 'very' 'vex' 'village' 'visit'\n",
            " 'visitor' 'visits' 'walk' 'walked' 'walking' 'want' 'wanted' 'was' 'way'\n",
            " 'ways' 'we' 'wedding' 'welcome' 'well' 'went' 'were' 'what' 'whatever'\n",
            " 'when' 'where' 'wherever' 'whether' 'which' 'while' 'who' 'whole' 'whom'\n",
            " 'why' 'widower' 'wife' 'will' 'willing' 'wish' 'wished' 'with' 'without'\n",
            " 'woman' 'word' 'work' 'world' 'worthy' 'would' 'year' 'years' 'yet' 'you'\n",
            " 'young' 'youngest' 'your' 'yourself']\n",
            "Vocabulary size 901 words\n"
          ]
        }
      ],
      "source": [
        "vocabulary = list(set(words)) #np.unique(words)\n",
        "vocabulary.sort()\n",
        "# get rid of some non-words like ',' '--' '['\n",
        "idx_first_word = vocabulary.index('A') # np.where(vocabulary=='A')[0].item()\n",
        "vocabulary = vocabulary[idx_first_word:]\n",
        "# plus some more annoying non-words\n",
        "vocabulary.remove('[')\n",
        "vocabulary.remove(']')\n",
        "vocabulary.remove('`')\n",
        "vocabulary.remove('II')\n",
        "vocabulary = np.array(vocabulary)\n",
        "print(vocabulary)\n",
        "print('Vocabulary size {} words'.format(len(vocabulary)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3CoMFvT7-Hk"
      },
      "source": [
        "For each word in the vocabulary find the nearest words = at Levenshtein distance up to ``MAX_DIST``. This is a long computation, $O(n^2)$ for $n$ size of the vocabulary. We try to speed up it a little : if $\\text{dist}(w_1, w_2) \\leq d$ then $|\\text{len}(w_1) - \\text{len}(w_2)| \\leq d$. This reduces the candidate words in the vocabulary for which to compute the distance to each word.\n",
        "We save the resulting dictionary to avoid recomputing it each time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQSBCKoC7-Hm",
        "outputId": "5ef5ae1d-cf29-4ba3-a659-2e6e5ac6dd63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 901/901 [00:21<00:00, 41.23it/s]\n"
          ]
        }
      ],
      "source": [
        "from nltk.metrics.distance import edit_distance\n",
        "\n",
        "def levenshtein(s1, s2):\n",
        "    return edit_distance(s1, s2, substitution_cost=1, transpositions=True)\n",
        "\n",
        "word_lengths = np.array([len(w) for w in vocabulary])\n",
        "dict_lengths = {}\n",
        "for l in range(min(word_lengths), max(word_lengths)+1):\n",
        "    dict_lengths[l] = vocabulary[word_lengths==l] # needs vocabulary to be a numpy array\n",
        "\n",
        "min_length = min(dict_lengths.keys())\n",
        "max_length = max(dict_lengths.keys())\n",
        "\n",
        "MAX_DIST = 1\n",
        "fname = 'close_words_{}.pkl'.format(MAX_DIST)\n",
        "if not os.path.exists(fname):\n",
        "    close_words = {}\n",
        "    for word in tqdm(vocabulary):\n",
        "        length = len(word)\n",
        "        candidate_words = []\n",
        "        d1 = max(min_length, length - MAX_DIST)\n",
        "        d2 = min(max_length, length + MAX_DIST)\n",
        "        for d in range(d1, d2+1):\n",
        "            candidate_words.extend(dict_lengths[d])\n",
        "        close_words[word] = [w for w in candidate_words if levenshtein(word,w) <= MAX_DIST]\n",
        "\n",
        "    with open(fname,'wb') as f:\n",
        "        pickle.dump(close_words, f)\n",
        "else:\n",
        "    close_words = pickle.load(open(fname,'rb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIdE_y_Z7-Ho"
      },
      "source": [
        "Given one sentence $X$, which is a list of words, build the candidates to correct sentence $C(X)$ assuming at most one word is mispelled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-GOhyl37-Hp",
        "outputId": "09960333-17d2-4434-9e16-10fb8631a67f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I\twish\tyou\twhere\there\n",
            "A\twish\tyou\twhere\there\n",
            "a\twish\tyou\twhere\there\n",
            "s\twish\tyou\twhere\there\n",
            "If\twish\tyou\twhere\there\n",
            "It\twish\tyou\twhere\there\n",
            "I\tfish\tyou\twhere\there\n",
            "I\twith\tyou\twhere\there\n",
            "I\twish\tYou\twhere\there\n",
            "I\twish\tyour\twhere\there\n",
            "I\twish\tyou\there\there\n",
            "I\twish\tyou\twere\there\n",
            "I\twish\tyou\tThere\there\n",
            "I\twish\tyou\tWhere\there\n",
            "I\twish\tyou\tthere\there\n",
            "I\twish\tyou\twhere\ther\n",
            "I\twish\tyou\twhere\thers\n",
            "I\twish\tyou\twhere\twere\n",
            "I\twish\tyou\twhere\tThere\n",
            "I\twish\tyou\twhere\tWhere\n",
            "I\twish\tyou\twhere\tthere\n",
            "I\twish\tyou\twhere\twhere\n"
          ]
        }
      ],
      "source": [
        "#sentence = 'Only two of the apples'\n",
        "sentence = 'I wish you where here'\n",
        "X = sentence.split(' ')\n",
        "for x in X:\n",
        "  assert x in vocabulary, 'All the words in the sentence must belong to the '\\\n",
        "      + 'vocabulary, {} doesn\\'t'.format(x)\n",
        "CX = [X] # no errors\n",
        "for i in range(len(X)): # one mispelled word at a time\n",
        "    if X[i] in vocabulary: \n",
        "        for cw in close_words[X[i]]:\n",
        "            if cw != X[i]:\n",
        "                C = X.copy()\n",
        "                C[i] = cw\n",
        "                CX.append(C)\n",
        "    else:\n",
        "        pass # let it be as is\n",
        "for W in CX:\n",
        "    print('\\t'.join(W))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gT9-f0od7-Hr"
      },
      "source": [
        "Likelihood $P(X | W) = \\prod_{i=1}^n p(x_i | w_i)$ where $n$ is number of words in $X$ (same as in $W$), and $p(x | w)$  is Eq. B.8. $X$ is the written sentence, $W$ are the candidate sentences in $C(X)$. Each $W$ contains zero (ie, $W=X$) or at most one mispelled word, and in this case the mispelled word $w_i$ is at a Levenshtein distance $1...$``MAX_DIST`` of the written word $x_i$ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WF4RPnLX7-Hs",
        "outputId": "87cc274e-68d0-47d2-ea97-70fd8e8f01a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence with highest likelihood is the written one, X\n",
            "I\twish\tyou\twhere\there \t 0.7737809374999999\n",
            "Likelihoods\n",
            "I\twish\tyou\twhere\there \t 0.7737809374999999\n",
            "A\twish\tyou\twhere\there \t 0.008145062500000006\n",
            "a\twish\tyou\twhere\there \t 0.008145062500000006\n",
            "s\twish\tyou\twhere\there \t 0.008145062500000006\n",
            "If\twish\tyou\twhere\there \t 0.008145062500000006\n",
            "It\twish\tyou\twhere\there \t 0.008145062500000006\n",
            "I\tfish\tyou\twhere\there \t 0.020362656250000017\n",
            "I\twith\tyou\twhere\there \t 0.020362656250000017\n",
            "I\twish\tYou\twhere\there \t 0.020362656250000017\n",
            "I\twish\tyour\twhere\there \t 0.020362656250000017\n",
            "I\twish\tyou\there\there \t 0.008145062500000006\n",
            "I\twish\tyou\twere\there \t 0.008145062500000006\n",
            "I\twish\tyou\tThere\there \t 0.008145062500000006\n",
            "I\twish\tyou\tWhere\there \t 0.008145062500000006\n",
            "I\twish\tyou\tthere\there \t 0.008145062500000006\n",
            "I\twish\tyou\twhere\ther \t 0.00581790178571429\n",
            "I\twish\tyou\twhere\thers \t 0.00581790178571429\n",
            "I\twish\tyou\twhere\twere \t 0.00581790178571429\n",
            "I\twish\tyou\twhere\tThere \t 0.00581790178571429\n",
            "I\twish\tyou\twhere\tWhere \t 0.00581790178571429\n",
            "I\twish\tyou\twhere\tthere \t 0.00581790178571429\n",
            "I\twish\tyou\twhere\twhere \t 0.00581790178571429\n"
          ]
        }
      ],
      "source": [
        "alpha = 0.95\n",
        "likelihoods = []\n",
        "for W in CX:\n",
        "    PXW = 1.0\n",
        "    #print(X,W)\n",
        "    for x,w in zip(X,W):\n",
        "        if w==x:\n",
        "            pxw = alpha\n",
        "        else:\n",
        "            close_to_x = close_words[x] # includes x itself\n",
        "            pxw = (1-alpha) / (len(close_to_x) - 1) # so we substract 1\n",
        "        PXW *= pxw\n",
        "    likelihoods.append(PXW)\n",
        "\n",
        "likelihoods = np.array(likelihoods)\n",
        "idx_most_likely = likelihoods.argmax()\n",
        "print('Sentence with highest likelihood is the written one, X')\n",
        "print('\\t'.join(CX[idx_most_likely]), '\\t', likelihoods[idx_most_likely])\n",
        "print('Likelihoods')\n",
        "num_candidates = len(CX)\n",
        "for i in range(num_candidates):\n",
        "    print('\\t'.join(CX[i]), '\\t', likelihoods[i])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fL_jue5E7-Hv"
      },
      "source": [
        "Priors $P(W)$ for all $W \\in C(X)$ computed by a LM model, for instance tri-grams (on this same corpus). We've chosen the stupid backoff version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64PapwMh7-Hw",
        "outputId": "67bcb661-a3b6-492f-ed0b-6facc35cc5f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "100%|██████████| 7717/7717 [00:00<00:00, 16093.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<NgramCounter with 3 ngram orders and 623754 ngrams>\n"
          ]
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "\n",
        "sents = nltk.corpus.gutenberg.sents('austen-emma.txt')\n",
        "text = []\n",
        "for s in tqdm(sents):\n",
        "    text.append(s[:-1]) # except ending point\n",
        "\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.lm.models import StupidBackoff\n",
        "\n",
        "n=3\n",
        "lm = StupidBackoff(alpha=0.4, order=n)\n",
        "train, vocab = padded_everygram_pipeline(n, text)\n",
        "lm.fit(train, vocab)\n",
        "print(lm.counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtUBjkAl7-Hy",
        "outputId": "c8914b19-7e99-42fe-f5e6-3b99e3248b22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X = ['I', 'wish', 'you', 'where', 'here']\n",
            "['<s>', '<s>', 'I', 'wish', 'you', 'where', 'here', '</s>']\n",
            "P(I | <s>, <s>) = 0.08422962291045744\n",
            "P(wish | <s>, I) = 0.016923076923076923\n",
            "P(you | I, wish) = 0.34375\n",
            "P(where | wish, you) = 0.0002385211687537269\n",
            "P(here | you, where) = 9.868527836390196e-05\n",
            "P(X) = 1.1533634670336262e-11\n",
            "\n",
            "W = CX[1] = ['A', 'wish', 'you', 'where', 'here']\n",
            "['<s>', '<s>', 'A', 'wish', 'you', 'where', 'here', '</s>']\n",
            "P(A | <s>, <s>) = 0.012051315277957756\n",
            "P(wish | <s>, A) = 9.942727293806665e-05\n",
            "P(you | A, wish) = 0.04477611940298508\n",
            "P(where | wish, you) = 0.0002385211687537269\n",
            "P(here | you, where) = 9.868527836390196e-05\n",
            "P(W) = 1.2628905903997896e-15\n"
          ]
        }
      ],
      "source": [
        "def P(W, verbose=False):\n",
        "    S = ['<s>', '<s>',] + W + ['</s>'] \n",
        "    if verbose: print(S)\n",
        "    num_words = len(S)\n",
        "    PW = 1.0\n",
        "    for i in range(2,num_words-1): # omit </s> because likelihoods don't have it\n",
        "        score = lm.score(S[i], [S[i-2], S[i-1]])\n",
        "        if verbose: print('P({} | {}, {}) = {}'.format(S[i], S[i-2], S[i-1], score))\n",
        "        PW *= score\n",
        "    return PW\n",
        "\n",
        "print('X =', X)\n",
        "score = P(X, verbose=True)\n",
        "print('P(X) = {}'.format(score))\n",
        "print('\\nW = CX[1] = {}'.format(CX[1]))\n",
        "score = P(CX[1], verbose=True)\n",
        "print('P(W) = {}'.format(score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbPXfszT7-Hz",
        "outputId": "5bcd6c41-3c22-45e8-b5fc-59da1f127b0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I\twish\tyou\twhere\there \tlikelihood=0.7737809374999999\tprior=1.1533634670336262e-11\tposterior=8.924506647995294e-12 \n",
            "A\twish\tyou\twhere\there \tlikelihood=0.008145062500000006\tprior=1.2628905903997896e-15\tposterior=1.0286322789468195e-17 \n",
            "a\twish\tyou\twhere\there \tlikelihood=0.008145062500000006\tprior=1.8186005327090402e-16\tposterior=1.4812615001448439e-18 \n",
            "s\twish\tyou\twhere\there \tlikelihood=0.008145062500000006\tprior=3.1281420915150617e-16\tposterior=2.547891284427092e-18 \n",
            "If\twish\tyou\twhere\there \tlikelihood=0.008145062500000006\tprior=1.004880684834241e-15\tposterior=8.184815983017701e-18 \n",
            "It\twish\tyou\twhere\there \tlikelihood=0.008145062500000006\tprior=4.005943270622988e-15\tposterior=3.262865831067868e-17 \n",
            "I\tfish\tyou\twhere\there \tlikelihood=0.020362656250000017\tprior=3.661075396645823e-18\tposterior=7.454921980723136e-20 \n",
            "I\twith\tyou\twhere\there \tlikelihood=0.020362656250000017\tprior=1.7653309328169103e-14\tposterior=3.594682695244262e-16 \n",
            "I\twish\tYou\twhere\there \tlikelihood=0.020362656250000017\tprior=1.6660920482470338e-15\tposterior=3.392605965931279e-17 \n",
            "I\twish\tyour\twhere\there \tlikelihood=0.020362656250000017\tprior=2.3158245592496206e-13\tposterior=4.715633943530782e-15 \n",
            "I\twish\tyou\there\there \tlikelihood=0.008145062500000006\tprior=4.771903281946753e-12\tposterior=3.8867450475411456e-14 \n",
            "I\twish\tyou\twere\there \tlikelihood=0.008145062500000006\tprior=2.3374580001296617e-07\tposterior=1.903874150218112e-09 <----\n",
            "I\twish\tyou\tThere\there \tlikelihood=0.008145062500000006\tprior=4.628387393767904e-12\tposterior=3.769850459645172e-14 \n",
            "I\twish\tyou\tWhere\there \tlikelihood=0.008145062500000006\tprior=5.74063552715399e-13\tposterior=4.675783515838973e-15 \n",
            "I\twish\tyou\tthere\there \tlikelihood=0.008145062500000006\tprior=3.4600904011008786e-11\tposterior=2.818265257261675e-13 \n",
            "I\twish\tyou\twhere\ther \tlikelihood=0.00581790178571429\tprior=2.0647807631632063e-10\tposterior=1.2012691689115732e-12 \n",
            "I\twish\tyou\twhere\thers \tlikelihood=0.00581790178571429\tprior=1.7343811534340247e-12\tposterior=1.0090459209673022e-14 \n",
            "I\twish\tyou\twhere\twere \tlikelihood=0.00581790178571429\tprior=5.125096308397542e-11\tposterior=2.9817306964583775e-13 \n",
            "I\twish\tyou\twhere\tThere \tlikelihood=0.00581790178571429\tprior=1.1186758439649458e-11\tposterior=6.508346190239099e-14 \n",
            "I\twish\tyou\twhere\tWhere \tlikelihood=0.00581790178571429\tprior=1.3875049227472196e-12\tposterior=8.072367367738417e-15 \n",
            "I\twish\tyou\twhere\tthere \tlikelihood=0.00581790178571429\tprior=1.316877746551922e-09\tposterior=7.661465393231837e-12 \n",
            "I\twish\tyou\twhere\twhere \tlikelihood=0.00581790178571429\tprior=6.157053094690786e-12\tposterior=3.582113019433922e-14 \n",
            "\n",
            "The original sentence was\n",
            "\tI wish you where here\n",
            "The right sentence is\n",
            "\tI wish you were here\n"
          ]
        }
      ],
      "source": [
        "priors = []\n",
        "for W in CX:\n",
        "    priors.append(P(W))\n",
        "    \n",
        "posteriors = np.array(priors)*np.array(likelihoods)\n",
        "idx_best_post = np.argmax(posteriors)\n",
        "\n",
        "for i in range(num_candidates):\n",
        "    best = '<----' if i==idx_best_post else ''\n",
        "    print('\\t'.join(CX[i]), '\\tlikelihood={}\\tprior={}\\tposterior={} {}'\n",
        "          .format(likelihoods[i], priors[i], posteriors[i],best))\n",
        "\n",
        "print('\\nThe original sentence was')\n",
        "print('\\t' + ' '.join(X))\n",
        "print('The right sentence is')\n",
        "print('\\t' + ' '.join(CX[idx_best_post]))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ngrams",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "a5f976fab99073dff0a6bed391851c6449ba69de3700e06fc73b9df6f40e665b"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}