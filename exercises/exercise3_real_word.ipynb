{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YQB5JHcy7-Hc"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SGTgkKv7-Hg",
        "outputId": "3182a7cc-4c0d-47c2-d4a6-5d881fb64fa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "words = None\n",
        "if False:\n",
        "    from nltk.corpus import gutenberg as corpus\n",
        "    words = corpus.words()\n",
        "    vocabulary = vocabulary[803:-2]\n",
        "    # first 803 and last 2 words are punctuation signs, numbers and underscored \n",
        "    # words like _home_\n",
        "else:\n",
        "    # Using the whole corpus means too much work to later compute the words at a \n",
        "    # distance for each word in the vocabulary, so we load just one book\n",
        "    import nltk\n",
        "    nltk.download('gutenberg')\n",
        "    words = nltk.corpus.gutenberg.words('austen-emma.txt')\n",
        "    # using all the words in the book will take 25-30 min. to process later\n",
        "    # so we limit its number for the moment.\n",
        "    words = words[:10000] # cell [15] will take 1.5 min. \n",
        "    #words = words[:100000] # cell [15] will take 15 min. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtZbq2hU7-Hj",
        "outputId": "df921f63-6e56-4aaf-e035-9e4ded790b15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['A' 'Abbey' 'After' ... 'yourself' 'youth' 'youthful']\n",
            "Vocabulary size 1711 words\n"
          ]
        }
      ],
      "source": [
        "vocabulary = list(set(words))\n",
        "vocabulary.sort()\n",
        "# get rid of some non-words like ',' '--' '['\n",
        "idx_first_word = vocabulary.index('A')\n",
        "vocabulary = vocabulary[idx_first_word:]\n",
        "# plus some more annoying non-words\n",
        "vocabulary.remove('[')\n",
        "vocabulary.remove(']')\n",
        "vocabulary.remove('`')\n",
        "vocabulary.remove('II')\n",
        "vocabulary = np.array(vocabulary)\n",
        "print(vocabulary)\n",
        "print('Vocabulary size {} words'.format(len(vocabulary)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3CoMFvT7-Hk"
      },
      "source": [
        "For each word in the vocabulary find the nearest words = at Levenshtein distance up to ``MAX_DIST``. This is a long computation, $O(n^2)$ for $n$ size of the vocabulary. We try to speed up it a little : if $\\text{dist}(w_1, w_2) \\leq d$ then $|\\text{len}(w_1) - \\text{len}(w_2)| \\leq d$. This reduces the candidate words in the vocabulary for which to compute the distance to each word.\n",
        "We save the resulting dictionary to avoid recomputing it each time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQSBCKoC7-Hm",
        "outputId": "0cc84f85-e392-4ae6-acb3-0ba5a19b7243"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1711/1711 [01:37<00:00, 17.52it/s]\n"
          ]
        }
      ],
      "source": [
        "from nltk.metrics.distance import edit_distance\n",
        "\n",
        "def levenshtein(s1, s2):\n",
        "    return edit_distance(s1, s2, substitution_cost=1, transpositions=True)\n",
        "\n",
        "word_lengths = np.array([len(w) for w in vocabulary])\n",
        "dict_lengths = {}\n",
        "for l in range(min(word_lengths), max(word_lengths)+1):\n",
        "    dict_lengths[l] = vocabulary[word_lengths==l] # needs vocabulary to be a numpy array\n",
        "\n",
        "min_length = min(dict_lengths.keys())\n",
        "max_length = max(dict_lengths.keys())\n",
        "\n",
        "MAX_DIST = 1\n",
        "fname = 'close_words_{}.pkl'.format(MAX_DIST)\n",
        "if not os.path.exists(fname):\n",
        "    close_words = {}\n",
        "    for word in tqdm(vocabulary):\n",
        "        length = len(word)\n",
        "        candidate_words = []\n",
        "        d1 = max(min_length, length - MAX_DIST)\n",
        "        d2 = min(max_length, length + MAX_DIST)\n",
        "        for d in range(d1, d2+1):\n",
        "            candidate_words.extend(dict_lengths[d])\n",
        "        close_words[word] = [w for w in candidate_words if levenshtein(word,w) <= MAX_DIST]\n",
        "\n",
        "    with open(fname,'wb') as f:\n",
        "        pickle.dump(close_words, f)\n",
        "else:\n",
        "    close_words = pickle.load(open(fname,'rb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIdE_y_Z7-Ho"
      },
      "source": [
        "Given one sentence $X$, which is a list of words, build the candidates to correct sentence $C(X)$ assuming at most one word is mispelled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-GOhyl37-Hp",
        "outputId": "60d8d1d4-a231-46ad-d9c9-d2508f41d7c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I\twish\tyou\twhere\there\n",
            "A\twish\tyou\twhere\there\n",
            "a\twish\tyou\twhere\there\n",
            "s\twish\tyou\twhere\there\n",
            "IV\twish\tyou\twhere\there\n",
            "If\twish\tyou\twhere\there\n",
            "In\twish\tyou\twhere\there\n",
            "It\twish\tyou\twhere\there\n",
            "I\tfish\tyou\twhere\there\n",
            "I\twith\tyou\twhere\there\n",
            "I\twish\tYou\twhere\there\n",
            "I\twish\tyour\twhere\there\n",
            "I\twish\tyou\there\there\n",
            "I\twish\tyou\twere\there\n",
            "I\twish\tyou\tThere\there\n",
            "I\twish\tyou\tWhere\there\n",
            "I\twish\tyou\tthere\there\n",
            "I\twish\tyou\twhere\ther\n",
            "I\twish\tyou\twhere\thers\n",
            "I\twish\tyou\twhere\twere\n",
            "I\twish\tyou\twhere\tThere\n",
            "I\twish\tyou\twhere\tWhere\n",
            "I\twish\tyou\twhere\tthere\n",
            "I\twish\tyou\twhere\twhere\n"
          ]
        }
      ],
      "source": [
        "#sentence = 'Only two of the apples'\n",
        "sentence = 'I wish you where here'\n",
        "X = sentence.split(' ')\n",
        "for x in X:\n",
        "  assert x in vocabulary, 'All the words in the sentence must belong to the '\\\n",
        "      + 'vocabulary, {} doesn\\'t'.format(x)\n",
        "CX = [X] # no errors\n",
        "for i in range(len(X)): # one mispelled word at a time\n",
        "    if X[i] in vocabulary: \n",
        "        for cw in close_words[X[i]]:\n",
        "            if cw != X[i]:\n",
        "                C = X.copy()\n",
        "                C[i] = cw\n",
        "                CX.append(C)\n",
        "    else:\n",
        "        pass # let it be as is\n",
        "for W in CX:\n",
        "    print('\\t'.join(W))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gT9-f0od7-Hr"
      },
      "source": [
        "Likelihood $P(X | W) = \\prod_{i=1}^n p(x_i | w_i)$ where $n$ is number of words in $X$ (same as in $W$), and $p(x | w)$  is Eq. B.8. $X$ is the written sentence, $W$ are the candidate sentences in $C(X)$. Each $W$ contains zero (ie, $W=X$) or at most one mispelled word, and in this case the mispelled word $w_i$ is at a Levenshtein distance $1...$ ``MAX_DIST`` of the written word $x_i$ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WF4RPnLX7-Hs",
        "outputId": "5dc6314b-1a60-4f7d-cef7-afb214aa4c4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence with highest likelihood is the written one, X\n",
            "I\twish\tyou\twhere\there \t 0.7737809374999999\n",
            "Likelihoods\n",
            "I\twish\tyou\twhere\there \t 0.7737809374999999\n",
            "A\twish\tyou\twhere\there \t 0.005817901785714291\n",
            "a\twish\tyou\twhere\there \t 0.005817901785714291\n",
            "s\twish\tyou\twhere\there \t 0.005817901785714291\n",
            "IV\twish\tyou\twhere\there \t 0.005817901785714291\n",
            "If\twish\tyou\twhere\there \t 0.005817901785714291\n",
            "In\twish\tyou\twhere\there \t 0.005817901785714291\n",
            "It\twish\tyou\twhere\there \t 0.005817901785714291\n",
            "I\tfish\tyou\twhere\there \t 0.020362656250000017\n",
            "I\twith\tyou\twhere\there \t 0.020362656250000017\n",
            "I\twish\tYou\twhere\there \t 0.020362656250000017\n",
            "I\twish\tyour\twhere\there \t 0.020362656250000017\n",
            "I\twish\tyou\there\there \t 0.008145062500000006\n",
            "I\twish\tyou\twere\there \t 0.008145062500000006\n",
            "I\twish\tyou\tThere\there \t 0.008145062500000006\n",
            "I\twish\tyou\tWhere\there \t 0.008145062500000006\n",
            "I\twish\tyou\tthere\there \t 0.008145062500000006\n",
            "I\twish\tyou\twhere\ther \t 0.00581790178571429\n",
            "I\twish\tyou\twhere\thers \t 0.00581790178571429\n",
            "I\twish\tyou\twhere\twere \t 0.00581790178571429\n",
            "I\twish\tyou\twhere\tThere \t 0.00581790178571429\n",
            "I\twish\tyou\twhere\tWhere \t 0.00581790178571429\n",
            "I\twish\tyou\twhere\tthere \t 0.00581790178571429\n",
            "I\twish\tyou\twhere\twhere \t 0.00581790178571429\n"
          ]
        }
      ],
      "source": [
        "alpha = 0.95\n",
        "likelihoods = []\n",
        "for W in CX:\n",
        "    PXW = 1.0\n",
        "    #print(X,W)\n",
        "    for x,w in zip(X,W):\n",
        "        if w==x:\n",
        "            pxw = alpha\n",
        "        else:\n",
        "            close_to_x = close_words[x] # includes x itself\n",
        "            pxw = (1-alpha) / (len(close_to_x) - 1) # so we substract 1\n",
        "        PXW *= pxw\n",
        "    likelihoods.append(PXW)\n",
        "\n",
        "likelihoods = np.array(likelihoods)\n",
        "idx_most_likely = likelihoods.argmax()\n",
        "print('Sentence with highest likelihood is the written one, X')\n",
        "print('\\t'.join(CX[idx_most_likely]), '\\t', likelihoods[idx_most_likely])\n",
        "print('Likelihoods')\n",
        "num_candidates = len(CX)\n",
        "for i in range(num_candidates):\n",
        "    print('\\t'.join(CX[i]), '\\t', likelihoods[i])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fL_jue5E7-Hv"
      },
      "source": [
        "Priors $P(W)$ for all $W \\in C(X)$ computed by a LM model, for instance tri-grams (on this same corpus). We've chosen the stupid backoff version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64PapwMh7-Hw",
        "outputId": "7d9d3192-9b6b-469c-8c18-4f6acd9ab695"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "100%|██████████| 7717/7717 [00:00<00:00, 10852.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<NgramCounter with 3 ngram orders and 623754 ngrams>\n"
          ]
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "\n",
        "sents = nltk.corpus.gutenberg.sents('austen-emma.txt')\n",
        "text = []\n",
        "for s in tqdm(sents):\n",
        "    text.append(s[:-1]) # except ending point\n",
        "\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.lm.models import StupidBackoff\n",
        "\n",
        "n=3\n",
        "lm = StupidBackoff(alpha=0.4, order=n)\n",
        "train, vocab = padded_everygram_pipeline(n, text)\n",
        "lm.fit(train, vocab)\n",
        "print(lm.counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtUBjkAl7-Hy",
        "outputId": "3471513b-2fac-422f-9ad6-f031926eea51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X = ['I', 'wish', 'you', 'where', 'here']\n",
            "['<s>', '<s>', 'I', 'wish', 'you', 'where', 'here', '</s>']\n",
            "P(I | <s>, <s>) = 0.08422962291045744\n",
            "P(wish | <s>, I) = 0.016923076923076923\n",
            "P(you | I, wish) = 0.34375\n",
            "P(where | wish, you) = 0.0002385211687537269\n",
            "P(here | you, where) = 9.868527836390196e-05\n",
            "P(X) = 1.1533634670336262e-11\n",
            "\n",
            "W = CX[1] = ['A', 'wish', 'you', 'where', 'here']\n",
            "['<s>', '<s>', 'A', 'wish', 'you', 'where', 'here', '</s>']\n",
            "P(A | <s>, <s>) = 0.012051315277957756\n",
            "P(wish | <s>, A) = 9.942727293806665e-05\n",
            "P(you | A, wish) = 0.04477611940298508\n",
            "P(where | wish, you) = 0.0002385211687537269\n",
            "P(here | you, where) = 9.868527836390196e-05\n",
            "P(W) = 1.2628905903997896e-15\n"
          ]
        }
      ],
      "source": [
        "def P(W, verbose=False):\n",
        "    S = ['<s>', '<s>',] + W + ['</s>'] \n",
        "    if verbose: print(S)\n",
        "    num_words = len(S)\n",
        "    PW = 1.0\n",
        "    for i in range(2,num_words-1): # omit </s> because likelihoods don't have it\n",
        "        score = lm.score(S[i], [S[i-2], S[i-1]])\n",
        "        if verbose: print('P({} | {}, {}) = {}'.format(S[i], S[i-2], S[i-1], score))\n",
        "        PW *= score\n",
        "    return PW\n",
        "\n",
        "print('X =', X)\n",
        "score = P(X, verbose=True)\n",
        "print('P(X) = {}'.format(score))\n",
        "print('\\nW = CX[1] = {}'.format(CX[1]))\n",
        "score = P(CX[1], verbose=True)\n",
        "print('P(W) = {}'.format(score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbPXfszT7-Hz",
        "outputId": "a2b23438-ed2d-428b-af98-5eb44bbf3c24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I\twish\tyou\twhere\there \tlikelihood=0.7737809374999999\tprior=1.1533634670336262e-11\tposterior=8.924506647995294e-12 \n",
            "A\twish\tyou\twhere\there \tlikelihood=0.005817901785714291\tprior=1.2628905903997896e-15\tposterior=7.347373421048711e-18 \n",
            "a\twish\tyou\twhere\there \tlikelihood=0.005817901785714291\tprior=1.8186005327090402e-16\tposterior=1.0580439286748885e-18 \n",
            "s\twish\tyou\twhere\there \tlikelihood=0.005817901785714291\tprior=3.1281420915150617e-16\tposterior=1.8199223460193514e-18 \n",
            "IV\twish\tyou\twhere\there \tlikelihood=0.005817901785714291\tprior=0.0\tposterior=0.0 \n",
            "If\twish\tyou\twhere\there \tlikelihood=0.005817901785714291\tprior=1.004880684834241e-15\tposterior=5.84629713072693e-18 \n",
            "In\twish\tyou\twhere\there \tlikelihood=0.005817901785714291\tprior=7.332913105547164e-16\tposterior=4.266216825125057e-18 \n",
            "It\twish\tyou\twhere\there \tlikelihood=0.005817901785714291\tprior=4.005943270622988e-15\tposterior=2.3306184507627628e-17 \n",
            "I\tfish\tyou\twhere\there \tlikelihood=0.020362656250000017\tprior=3.661075396645823e-18\tposterior=7.454921980723136e-20 \n",
            "I\twith\tyou\twhere\there \tlikelihood=0.020362656250000017\tprior=1.7653309328169103e-14\tposterior=3.594682695244262e-16 \n",
            "I\twish\tYou\twhere\there \tlikelihood=0.020362656250000017\tprior=1.6660920482470338e-15\tposterior=3.392605965931279e-17 \n",
            "I\twish\tyour\twhere\there \tlikelihood=0.020362656250000017\tprior=2.3158245592496206e-13\tposterior=4.715633943530782e-15 \n",
            "I\twish\tyou\there\there \tlikelihood=0.008145062500000006\tprior=4.771903281946753e-12\tposterior=3.8867450475411456e-14 \n",
            "I\twish\tyou\twere\there \tlikelihood=0.008145062500000006\tprior=2.3374580001296617e-07\tposterior=1.903874150218112e-09 <----\n",
            "I\twish\tyou\tThere\there \tlikelihood=0.008145062500000006\tprior=4.628387393767904e-12\tposterior=3.769850459645172e-14 \n",
            "I\twish\tyou\tWhere\there \tlikelihood=0.008145062500000006\tprior=5.74063552715399e-13\tposterior=4.675783515838973e-15 \n",
            "I\twish\tyou\tthere\there \tlikelihood=0.008145062500000006\tprior=3.4600904011008786e-11\tposterior=2.818265257261675e-13 \n",
            "I\twish\tyou\twhere\ther \tlikelihood=0.00581790178571429\tprior=2.0647807631632063e-10\tposterior=1.2012691689115732e-12 \n",
            "I\twish\tyou\twhere\thers \tlikelihood=0.00581790178571429\tprior=1.7343811534340247e-12\tposterior=1.0090459209673022e-14 \n",
            "I\twish\tyou\twhere\twere \tlikelihood=0.00581790178571429\tprior=5.125096308397542e-11\tposterior=2.9817306964583775e-13 \n",
            "I\twish\tyou\twhere\tThere \tlikelihood=0.00581790178571429\tprior=1.1186758439649458e-11\tposterior=6.508346190239099e-14 \n",
            "I\twish\tyou\twhere\tWhere \tlikelihood=0.00581790178571429\tprior=1.3875049227472196e-12\tposterior=8.072367367738417e-15 \n",
            "I\twish\tyou\twhere\tthere \tlikelihood=0.00581790178571429\tprior=1.316877746551922e-09\tposterior=7.661465393231837e-12 \n",
            "I\twish\tyou\twhere\twhere \tlikelihood=0.00581790178571429\tprior=6.157053094690786e-12\tposterior=3.582113019433922e-14 \n",
            "\n",
            "The original sentence was\n",
            "\tI wish you where here\n",
            "The right sentence is\n",
            "\tI wish you were here\n"
          ]
        }
      ],
      "source": [
        "priors = []\n",
        "for W in CX:\n",
        "    priors.append(P(W))\n",
        "    \n",
        "posteriors = np.array(priors)*np.array(likelihoods)\n",
        "idx_best_post = np.argmax(posteriors)\n",
        "\n",
        "for i in range(num_candidates):\n",
        "    best = '<----' if i==idx_best_post else ''\n",
        "    print('\\t'.join(CX[i]), '\\tlikelihood={}\\tprior={}\\tposterior={} {}'\n",
        "          .format(likelihoods[i], priors[i], posteriors[i],best))\n",
        "\n",
        "print('\\nThe original sentence was')\n",
        "print('\\t' + ' '.join(X))\n",
        "print('The right sentence is')\n",
        "print('\\t' + ' '.join(CX[idx_best_post]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.log(posteriors) - np.log(posteriors[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69iTLqTgCgSe",
        "outputId": "5b169ada-fa3b-4dd0-a9cd-77c360c0fe8b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  0.         -14.00996871 -15.94788975 -15.40551777         -inf\n",
            " -14.23850311 -14.55358416 -12.85559285 -18.60060731 -10.1196857\n",
            " -12.48012817  -7.54567297  -5.43639918   5.36284507  -5.4669359\n",
            "  -7.55415958  -3.4552646   -2.00542241  -6.78496598  -3.39888224\n",
            "  -4.92088585  -7.00810953  -0.15259778  -5.51801838]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-86a724af623e>:1: RuntimeWarning: divide by zero encountered in log\n",
            "  print(np.log(posteriors) - np.log(posteriors[0]))\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ngrams",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "a5f976fab99073dff0a6bed391851c6449ba69de3700e06fc73b9df6f40e665b"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}