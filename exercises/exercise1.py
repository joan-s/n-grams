# -*- coding: utf-8 -*-
"""exercise1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19BaOk3-2Lh_QKz44WnX35pMKJ8uWcsyI
"""

from tqdm import tqdm
import nltk
nltk.download('punkt')
nltk.download('gutenberg')
nltk.download('cess_cat')

"""Load a corpus in Catalan or English. The nltk corpora result from tokenizing and segmenting into sentences large collections of text.

The ``gutenberg`` corpus comes from a set of English literature classics. The ``cess_cat`` corpus comes from https://www.cs.upc.edu/~nlp/wikicorpus/, the "120 Million Word Spanish Corpus" which has a subset in Catalan of 50 million words scrapped from Vikipedia in 2006.
"""

name_corpus = 'cess_cat'

if name_corpus=='cess_cat':
    from nltk.corpus import cess_cat as corpus
    # clean the corpus of strange words
    words = []
    words_to_remove = ['*0*', '-Fpa-', '-Fpt-']
    for w in tqdm(corpus.words()):
        if w not in words_to_remove:
            words.append(w)

elif name_corpus=='gutenberg':
    from nltk.corpus import gutenberg as corpus
    print(corpus.fileids())
    words = corpus.words()
else:
    assert False

print('corpus {} : {} words, {} sentences'
      .format(name_corpus, len(words), len(corpus.sents())))

"""Build a language model from bigrams. A LM is just a dictionary
with key = condition = one word, and value = ``FreqDist`` 
object = another dictionary with key = next word, value = number 
of occurrences.
This is adapted from https://www.nltk.org/book/ch02.html, section 2.4

"""

grams = list(nltk.bigrams(words))
# also trigrams, ngrams, everygrams(max_len)
cfd = nltk.ConditionalFreqDist(grams)
print(cfd.conditions())
for i in [100, 200, 300, 400]:
    print(cfd.conditions()[i])
    print(cfd[cfd.conditions()[i]].most_common())
    print('--------------')

if name_corpus == 'cess_cat':
    print(cfd['Una'])
else:
    print(cfd['The'])

"""Sample text from the language model"""

import random

def sample_bigram_model(cfd_bigrams, last_word, num=15):
    for i in range(num):
        print(last_word, end=' ')
        # if we do w_k = \arg \max w \in V p(w | w_{k-1}) with
        #     next_word = cfdist[word].max()
        # we get caught in a cycle, repeating again and again 
        # the same few words. It is better to sample from the
        # probability distribution with
        next_word = random.choice(list(cfd_bigrams[last_word].elements()))
        last_word = next_word


if name_corpus=='cess_cat':
    print(sample_bigram_model(cfd, 'El', 100))
    print(sample_bigram_model(cfd, 'La', 100))
    print(sample_bigram_model(cfd, 'Per', 100))
else:
    print(sample_bigram_model(cfd, 'The', 100))
    print(sample_bigram_model(cfd, 'For', 100))

"""Extension of previous function to tri, 4... n-grams is long and complicated
because conditions of cfd are not one word but lists of pairs, triplets, n-1 words. In addition, the probability of not finding the previous 2, 3..n
generated words among the conditions (ngrams) is very high. So better rely
on the ``lm`` package of nltk. It has also support for adding ``<s>``, ``</s>`` symbols to sentences (padding), different types of smoothing and backoff, and sampling text.

Build a proper language model with support for ``<s>``, ``</s>``, smoothing, backoff, sampling and computation of perplexity. See how here
https://www.nltk.org/api/nltk.lm.html
"""

if name_corpus=='cess_cat':
    text = []
    words_to_remove = ['*0*', '-Fpa-', '-Fpt-']
    #for s in tqdm(corpus.sents()[:1000]): # debug or quickly train the network
    for s in tqdm(corpus.sents()):
        new_s = [w for w in s if w not in words_to_remove]
        text.append(new_s[:-1]) # except ending point
else:
    text = []
    for s in tqdm(corpus.sents()):
        text.append(s[:-1]) # except ending point
    
from nltk.lm.preprocessing import padded_everygram_pipeline
from nltk.lm.models import MLE, Laplace, StupidBackoff

n = 3
lm1 = MLE(n)
lm2 = Laplace(n)
lm3 = StupidBackoff(alpha=0.4, order=n)
for lm in [lm1, lm2, lm3]:
    print(lm)
    train, vocab = padded_everygram_pipeline(3, text)
    # can not reuse the same pair of train, vocab!
    lm.fit(train, vocab)

    #print(lm.vocab.lookup(text[0]))
    #print(lm.vocab.lookup(['beeeee', 'muuuu']))
    print(lm.counts)
    #print(lm.score('El'), lm.score('el'), lm.score('dia'), lm.score("<UNK>"))
    print(lm.perplexity([('relaci√≥', 'amb', 'les', 'empreses')]))
    print(' '.join(lm.generate(100, random_seed=4)))

